# -*- coding: utf-8 -*-
"""classification_utils.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGDnDUcQnejNwuA1HEftYG1wcyNfweop

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ï¼š

åˆ†é¡ã‚¿ã‚¹ã‚¯ï¼ˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰

Google Driveé€£æºå®Œå‚™

XGBoost / RandomForest / Logistic / SVMå¯¾å¿œ

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½

ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆAccuracyã€F1ã€ROCã€PRã€æ··åŒè¡Œåˆ—ï¼‰

SHAPè§£æï¼ˆSummary, Waterfall, 3Dï¼‰

ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ”¯æ´ã‚ã‚Š
"""

# ================================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ç’°å¢ƒè¨­å®š
# ================================

import os
import joblib
import logging
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,
                             roc_curve, auc, precision_recall_curve, f1_score)
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from google.colab import drive
from google.colab import files

from typing import Any, Dict, List, Optional, Tuple, Union

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)
# ================================
# Google Driveé€£æºãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ»èª­è¾¼
# ================================

def mount_drive() -> None:
    """Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã™ã‚‹"""
    try:
        drive.mount('/content/drive')
        logger.info("âœ… Google Driveãƒã‚¦ãƒ³ãƒˆæˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Driveãƒã‚¦ãƒ³ãƒˆå¤±æ•—: {e}")

def save_to_drive(data: Any, relative_path: str) -> None:
    """ãƒ‡ãƒ¼ã‚¿ã‚’Google Driveã«ä¿å­˜ã™ã‚‹"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        joblib.dump(data, full_path)
        logger.info(f"âœ… ä¿å­˜æˆåŠŸ: {full_path}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å¤±æ•—: {e}")

def load_from_drive(relative_path: str) -> Any:
    """Google Driveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        return joblib.load(full_path)
    except Exception as e:
        logger.error(f"âŒ ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None
# ================================
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ãƒ»åˆ†å‰²
# ================================

def load_dataset(dataset_name: str) -> Tuple[pd.DataFrame, pd.Series, Optional[np.ndarray]]:
    """
    æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆdigits, iris, breast_cancer, wineå¯¾å¿œï¼‰
    """
    from sklearn import datasets
    try:
        if dataset_name == 'digits':
            data = datasets.load_digits()
            return pd.DataFrame(data.data), pd.Series(data.target), data.images
        elif dataset_name == 'iris':
            data = datasets.load_iris()
            return pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target), None
        elif dataset_name == 'breast_cancer':
            data = datasets.load_breast_cancer()
            return pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target), None
        elif dataset_name == 'wine':
            data = datasets.load_wine()
            return pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target), None
        else:
            logger.error(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
            return None, None, None
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None, None, None

def load_csv_data(file_path: str, target_column: str, features_to_drop: Optional[List[str]] = None) -> Tuple[pd.DataFrame, pd.Series]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    try:
        if not os.path.exists(file_path) and '/content/drive' not in file_path:
            file_path = os.path.join('/content/drive/MyDrive/', file_path)

        df = pd.read_csv(file_path)
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:\n{df.describe().T}")

        if features_to_drop:
            df = df.drop(columns=features_to_drop)

        if target_column not in df.columns:
            logger.error(f"ç›®çš„å¤‰æ•° '{target_column}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return None, None

        X = df.drop(columns=[target_column])
        y = df[target_column]

        cat_columns = X.select_dtypes(include=['object']).columns
        if len(cat_columns) > 0:
            logger.info(f"ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°æ¤œå‡º: {list(cat_columns)}")

        return X, y
    except Exception as e:
        logger.error(f"CSVãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None, None

def preprocess_and_split(X: pd.DataFrame, y: pd.Series,
                          test_size: float = 0.2,
                          validation_size: float = 0.0,
                          random_state: int = 42) -> Union[
                              Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series],
                              Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²ã™ã‚‹
    """
    if X is None or y is None:
        raise ValueError("å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒNoneã§ã™")

    categorical_cols = X.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        logger.info(f"ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰: {list(categorical_cols)}")
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

    if X.isnull().sum().sum() > 0:
        logger.info("æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ")
        X = X.fillna(X.median())

    if validation_size > 0:
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size + validation_size, random_state=random_state)
        valid_ratio = validation_size / (test_size + validation_size)
        X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=1 - valid_ratio, random_state=random_state)
        return X_train, X_valid, X_test, y_train, y_valid, y_test
    else:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
        return X_train, X_test, y_train, y_test
# ================================
# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# ================================

def train_model(X_train: pd.DataFrame, y_train: pd.Series,
                model_type: str = "xgboost",
                params: Optional[Dict] = None,
                use_grid_search: bool = False,
                param_grid: Optional[Dict] = None,
                cv: int = 5) -> Any:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã™ã‚‹
    """
    logger.info(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹: {model_type}")

    if model_type.lower() == "xgboost":
        model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
    elif model_type.lower() == "random_forest":
        model = RandomForestClassifier()
    elif model_type.lower() == "logistic":
        model = LogisticRegression(max_iter=1000)
    elif model_type.lower() == "svm":
        model = SVC(probability=True)
    else:
        logger.error(f"âŒ æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
        return None

    if params:
        model.set_params(**params)

    if use_grid_search and param_grid:
        logger.info("ğŸ” GridSearchCVã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
        model = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1, verbose=1)

    model.fit(X_train, y_train)
    logger.info("âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
    return model

def perform_cross_validation(model: Any, X: pd.DataFrame, y: pd.Series, cv: int = 5) -> Dict[str, float]:
    """
    ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹
    """
    logger.info(f"ğŸ”„ {cv}-foldã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ")
    scores = cross_val_score(model, X, y, cv=cv)
    return {
        "mean_score": np.mean(scores),
        "std_score": np.std(scores),
        "scores": scores.tolist()
    }
# ================================
# ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
# ================================

def evaluate_model(model: Any, X_test: pd.DataFrame, y_test: pd.Series,
                   plot_confusion: bool = True, plot_roc: bool = True, plot_pr: bool = True) -> None:
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹
    """
    preds = model.predict(X_test)
    probas = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, preds)
    f1 = f1_score(y_test, preds, average="macro")

    logger.info(f"ğŸ¯ Accuracy: {acc:.4f}")
    logger.info(f"ğŸ¯ F1 Score (Macro): {f1:.4f}")
    print(classification_report(y_test, preds))

    if plot_confusion:
        cm = confusion_matrix(y_test, preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title("Confusion Matrix")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

    if probas is not None and plot_roc:
        fpr, tpr, thresholds = roc_curve(y_test, probas)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
        plt.plot([0, 1], [0, 1], linestyle='--')
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title("ROC Curve")
        plt.legend()
        plt.show()

    if probas is not None and plot_pr:
        precision, recall, thresholds = precision_recall_curve(y_test, probas)
        plt.plot(recall, precision)
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.title("Precision-Recall Curve")
        plt.show()
# ================================
# SHAPè§£æ
# ================================

def explain_shap(model: Any, X_test: pd.DataFrame, model_type: str = "tree",
                 plot_summary: bool = True, plot_waterfall: bool = True, row_index: int = 0) -> shap.Explanation:
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹SHAPè§£æã‚’è¡Œã†
    """
    try:
        if model_type == "tree":
            explainer = shap.TreeExplainer(model)
        else:
            explainer = shap.Explainer(model.predict, X_test)

        shap_values = explainer(X_test)

        if plot_summary:
            shap.summary_plot(shap_values.values, features=X_test, feature_names=X_test.columns)

        if plot_waterfall:
            shap.plots.waterfall(shap_values[row_index])

        return shap_values
    except Exception as e:
        logger.error(f"âŒ SHAPè§£æå¤±æ•—: {e}")
        return None

def plot_shap_3d(shap_values: shap.Explanation, X_test: pd.DataFrame,
                 feature_x: str, feature_y: str, shap_feature: Optional[str] = None) -> None:
    """
    2è»¸ç‰¹å¾´é‡ã¨SHAPå€¤ã‚’3Dãƒ—ãƒ­ãƒƒãƒˆ
    """
    try:
        if shap_feature:
            shap_z = shap_values.values[:, X_test.columns.get_loc(shap_feature)]
        else:
            shap_z = shap_values.values.mean(axis=1)

        df = pd.DataFrame({
            "X": X_test[feature_x],
            "Y": X_test[feature_y],
            "SHAP": shap_z
        })

        fig = px.scatter_3d(df, x="X", y="Y", z="SHAP", color="SHAP", opacity=0.7)
        fig.show()
    except Exception as e:
        logger.error(f"âŒ 3D SHAPãƒ—ãƒ­ãƒƒãƒˆå¤±æ•—: {e}")
# ================================
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ”¯æ´
# ================================

def upload_file_from_local() -> Dict[str, Any]:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«PCã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    logger.info("ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    uploaded = files.upload()
    logger.info(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«: {list(uploaded.keys())}")
    return uploaded