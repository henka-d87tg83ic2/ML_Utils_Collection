# -*- coding: utf-8 -*-
"""classification_utils.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGDnDUcQnejNwuA1HEftYG1wcyNfweop

„Åì„ÅÆ„Éï„Ç°„Ç§„É´„ÅØÔºö

ÂàÜÈ°û„Çø„Çπ„ÇØÔºàÂàÜÈ°û„É¢„Éá„É´Áî®Ôºâ

Google DriveÈÄ£Êê∫ÂÆåÂÇô

XGBoost / RandomForest / Logistic / SVMÂØæÂøú

„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ÂèØËÉΩ

„É¢„Éá„É´Ë©ï‰æ°ÔºàAccuracy„ÄÅF1„ÄÅROC„ÄÅPR„ÄÅÊ∑∑ÂêåË°åÂàóÔºâ

SHAPËß£ÊûêÔºàSummary, Waterfall, 3DÔºâ

„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÊîØÊè¥„ÅÇ„Çä
"""

# ================================
# „Ç§„É≥„Éù„Éº„Éà„Å®Áí∞Â¢ÉË®≠ÂÆö
# ================================

import os
import joblib
import logging
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,
                             roc_curve, auc, precision_recall_curve, f1_score)
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from google.colab import drive
from google.colab import files

from typing import Any, Dict, List, Optional, Tuple, Union

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)
# ================================
# Google DriveÈÄ£Êê∫„Éª„Éï„Ç°„Ç§„É´‰øùÂ≠ò„ÉªË™≠Ëæº
# ================================

def mount_drive() -> None:
    """Google Drive„Çí„Éû„Ç¶„É≥„Éà„Åô„Çã"""
    try:
        drive.mount('/content/drive')
        logger.info("‚úÖ Google Drive„Éû„Ç¶„É≥„ÉàÊàêÂäü")
    except Exception as e:
        logger.error(f"‚ùå Drive„Éû„Ç¶„É≥„ÉàÂ§±Êïó: {e}")

def save_to_drive(data: Any, relative_path: str) -> None:
    """„Éá„Éº„Çø„ÇíGoogle Drive„Å´‰øùÂ≠ò„Åô„Çã"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        joblib.dump(data, full_path)
        logger.info(f"‚úÖ ‰øùÂ≠òÊàêÂäü: {full_path}")
    except Exception as e:
        logger.error(f"‚ùå ‰øùÂ≠òÂ§±Êïó: {e}")

def load_from_drive(relative_path: str) -> Any:
    """Google Drive„Åã„Çâ„Éá„Éº„Çø„Çí„É≠„Éº„Éâ„Åô„Çã"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        return joblib.load(full_path)
    except Exception as e:
        logger.error(f"‚ùå „É≠„Éº„ÉâÂ§±Êïó: {e}")
        return None
# ================================
# „Éá„Éº„Çø„É≠„Éº„Éâ„ÉªÂâçÂá¶ÁêÜ„ÉªÂàÜÂâ≤
# ================================

def load_dataset(dataset_name: str) -> Tuple[pd.DataFrame, pd.Series, Optional[np.ndarray]]:
    """
    Ê®ôÊ∫ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„É≠„Éº„Éâ„Åô„ÇãÔºàdigits, iris, breast_cancer, wineÂØæÂøúÔºâ
    """
    from sklearn import datasets
    try:
        if dataset_name == 'digits':
            data = datasets.load_digits()
            return pd.DataFrame(data.data), pd.Series(data.target), data.images
        elif dataset_name == 'iris':
            data = datasets.load_iris()
            return pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target), None
        elif dataset_name == 'breast_cancer':
            data = datasets.load_breast_cancer()
            return pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target), None
        elif dataset_name == 'wine':
            data = datasets.load_wine()
            return pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target), None
        else:
            logger.error(f"Êú™ÂØæÂøú„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà: {dataset_name}")
            return None, None, None
    except Exception as e:
        logger.error(f"„Éá„Éº„Çø„Çª„ÉÉ„Éà„É≠„Éº„ÉâÂ§±Êïó: {e}")
        return None, None, None

def load_csv_data(file_path: str, target_column: str, features_to_drop: Optional[List[str]] = None) -> Tuple[pd.DataFrame, pd.Series]:
    """
    CSV„Éï„Ç°„Ç§„É´„Åã„Çâ„Éá„Éº„Çø„Çí„É≠„Éº„Éâ„Åô„Çã
    """
    try:
        if not os.path.exists(file_path) and '/content/drive' not in file_path:
            file_path = os.path.join('/content/drive/MyDrive/', file_path)

        df = pd.read_csv(file_path)
        logger.info(f"üìä „Éá„Éº„ÇøÊ¶ÇË¶Å:\n{df.describe().T}")

        if features_to_drop:
            df = df.drop(columns=features_to_drop)

        if target_column not in df.columns:
            logger.error(f"ÁõÆÁöÑÂ§âÊï∞ '{target_column}' „ÅåÂ≠òÂú®„Åó„Åæ„Åõ„Çì")
            return None, None

        X = df.drop(columns=[target_column])
        y = df[target_column]

        cat_columns = X.select_dtypes(include=['object']).columns
        if len(cat_columns) > 0:
            logger.info(f"„Ç´„ÉÜ„Ç¥„É™Â§âÊï∞Ê§úÂá∫: {list(cat_columns)}")

        return X, y
    except Exception as e:
        logger.error(f"CSV„É≠„Éº„ÉâÂ§±Êïó: {e}")
        return None, None

def preprocess_and_split(X: pd.DataFrame, y: pd.Series,
                          test_size: float = 0.2,
                          validation_size: float = 0.0,
                          random_state: int = 42) -> Union[
                              Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series],
                              Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]]:
    """
    „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíË®ìÁ∑¥„ÉªÊ§úË®º„Éª„ÉÜ„Çπ„Éà„Å´ÂàÜÂâ≤„Åô„Çã
    """
    if X is None or y is None:
        raise ValueError("ÂÖ•Âäõ„Éá„Éº„Çø„ÅåNone„Åß„Åô")

    categorical_cols = X.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        logger.info(f"„Ç´„ÉÜ„Ç¥„É™Â§âÊï∞„Ç®„É≥„Ç≥„Éº„Éâ: {list(categorical_cols)}")
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

    if X.isnull().sum().sum() > 0:
        logger.info("Ê¨†ÊêçÂÄ§„Çí‰∏≠Â§ÆÂÄ§„ÅßË£úÂÆå")
        X = X.fillna(X.median())

    if validation_size > 0:
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size + validation_size, random_state=random_state)
        valid_ratio = validation_size / (test_size + validation_size)
        X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=1 - valid_ratio, random_state=random_state)
        return X_train, X_valid, X_test, y_train, y_valid, y_test
    else:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
        return X_train, X_test, y_train, y_test
# ================================
# „É¢„Éá„É´Â≠¶Áøí„Éª„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞
# ================================

def train_model(X_train: pd.DataFrame, y_train: pd.Series,
                model_type: str = "xgboost",
                params: Optional[Dict] = None,
                use_grid_search: bool = False,
                param_grid: Optional[Dict] = None,
                cv: int = 5) -> Any:
    """
    ÊåáÂÆö„Åï„Çå„Åü„É¢„Éá„É´„ÅßÂ≠¶Áøí„Åô„Çã
    """
    logger.info(f"üîÑ „É¢„Éá„É´Â≠¶ÁøíÈñãÂßã: {model_type}")

    if model_type.lower() == "xgboost":
        model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
    elif model_type.lower() == "random_forest":
        model = RandomForestClassifier()
    elif model_type.lower() == "logistic":
        model = LogisticRegression(max_iter=1000)
    elif model_type.lower() == "svm":
        model = SVC(probability=True)
    else:
        logger.error(f"‚ùå Êú™ÂØæÂøú„ÅÆ„É¢„Éá„É´„Çø„Ç§„Éó: {model_type}")
        return None

    if params:
        model.set_params(**params)

    if use_grid_search and param_grid:
        logger.info("üîç GridSearchCV„Åß„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞")
        model = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1, verbose=1)

    model.fit(X_train, y_train)
    logger.info("‚úÖ „É¢„Éá„É´Â≠¶ÁøíÂÆå‰∫Ü")
    return model

def perform_cross_validation(model: Any, X: pd.DataFrame, y: pd.Series, cv: int = 5) -> Dict[str, float]:
    """
    „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó„Åô„Çã
    """
    logger.info(f"üîÑ {cv}-fold„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥ÂÆüË°å")
    scores = cross_val_score(model, X, y, cv=cv)
    return {
        "mean_score": np.mean(scores),
        "std_score": np.std(scores),
        "scores": scores.tolist()
    }
# ================================
# „É¢„Éá„É´Ë©ï‰æ°
# ================================

def evaluate_model(model: Any, X_test: pd.DataFrame, y_test: pd.Series,
                   plot_confusion: bool = True, plot_roc: bool = True, plot_pr: bool = True) -> None:
    """
    „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Å¶„É¢„Éá„É´„ÇíË©ï‰æ°„Åô„Çã
    """
    preds = model.predict(X_test)
    probas = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, preds)
    f1 = f1_score(y_test, preds, average="macro")

    logger.info(f"üéØ Accuracy: {acc:.4f}")
    logger.info(f"üéØ F1 Score (Macro): {f1:.4f}")
    print(classification_report(y_test, preds))

    if plot_confusion:
        cm = confusion_matrix(y_test, preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title("Confusion Matrix")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

    if probas is not None and plot_roc:
        fpr, tpr, thresholds = roc_curve(y_test, probas)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
        plt.plot([0, 1], [0, 1], linestyle='--')
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title("ROC Curve")
        plt.legend()
        plt.show()

    if probas is not None and plot_pr:
        precision, recall, thresholds = precision_recall_curve(y_test, probas)
        plt.plot(recall, precision)
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.title("Precision-Recall Curve")
        plt.show()
# ================================
# SHAPËß£Êûê
# ================================

def explain_shap(model: Any, X_test: pd.DataFrame, model_type: str = "tree",
                 plot_summary: bool = True, plot_waterfall: bool = True, row_index: int = 0) -> shap.Explanation:
    """
    „ÉÜ„Çπ„Éà„Éá„Éº„Çø„Å´ÂØæ„Åô„ÇãSHAPËß£Êûê„ÇíË°å„ÅÜ
    """
    try:
        if model_type == "tree":
            explainer = shap.TreeExplainer(model)
        else:
            explainer = shap.Explainer(model.predict, X_test)

        shap_values = explainer(X_test)

        if plot_summary:
            shap.summary_plot(shap_values.values, features=X_test, feature_names=X_test.columns)

        if plot_waterfall:
            shap.plots.waterfall(shap_values[row_index])

        return shap_values
    except Exception as e:
        logger.error(f"‚ùå SHAPËß£ÊûêÂ§±Êïó: {e}")
        return None

def plot_shap_3d(shap_values: shap.Explanation, X_test: pd.DataFrame,
                 feature_x: str, feature_y: str, shap_feature: Optional[str] = None) -> None:
    """
    2Ëª∏ÁâπÂæ¥Èáè„Å®SHAPÂÄ§„Çí3D„Éó„É≠„ÉÉ„Éà
    """
    try:
        if shap_feature:
            shap_z = shap_values.values[:, X_test.columns.get_loc(shap_feature)]
        else:
            shap_z = shap_values.values.mean(axis=1)

        df = pd.DataFrame({
            "X": X_test[feature_x],
            "Y": X_test[feature_y],
            "SHAP": shap_z
        })

        fig = px.scatter_3d(df, x="X", y="Y", z="SHAP", color="SHAP", opacity=0.7)
        fig.show()
    except Exception as e:
        logger.error(f"‚ùå 3D SHAP„Éó„É≠„ÉÉ„ÉàÂ§±Êïó: {e}")
# ================================
# „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÊîØÊè¥
# ================================

def upload_file_from_local() -> Dict[str, Any]:
    """
    „É≠„Éº„Ç´„É´PC„Åã„Çâ„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åô„Çã
    """
    logger.info("üìÇ „É≠„Éº„Ç´„É´„Åã„Çâ„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
    uploaded = files.upload()
    logger.info(f"‚úÖ „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Éï„Ç°„Ç§„É´: {list(uploaded.keys())}")
    return uploaded