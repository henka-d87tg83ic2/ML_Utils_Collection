# -*- coding: utf-8 -*-
"""reinforcement_learning_utils.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SOpNI_erNMh6kJR6KzX2BkQLfpzRzKb_

1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ»ç’°å¢ƒè¨­å®šï¼ˆãƒ­ã‚®ãƒ³ã‚°å«ã‚€ï¼‰
2. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆä¾‹ï¼šOpenAI Gymç’°å¢ƒï¼‰
3. Q-Learningï¼ˆç°¡å˜ãªè¡¨å½¢å¼ï¼‰
4. ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­è¾¼ï¼ˆGoogle Driveå¯¾å¿œï¼‰
5. å­¦ç¿’ãƒ­ã‚°ãƒ»å ±é…¬æ¨ç§»ã®å¯è¦–åŒ–
6. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ”¯æ´
"""

# ================================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ç’°å¢ƒè¨­å®š
# ================================

import os
import joblib
import logging
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import gym
from google.colab import drive
from google.colab import files

from typing import Any, Dict, List, Optional, Tuple, Union

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)
# ================================
# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ================================

def create_gym_environment(env_name: str = "FrozenLake-v1") -> gym.Env:
    """OpenAI Gymã®ç’°å¢ƒã‚’ä½œæˆã™ã‚‹"""
    try:
        env = gym.make(env_name)
        logger.info(f"âœ… ç’°å¢ƒä½œæˆæˆåŠŸ: {env_name}")
        return env
    except Exception as e:
        logger.error(f"âŒ ç’°å¢ƒä½œæˆå¤±æ•—: {e}")
        return None
# ================================
# Q-Learningå®Ÿè£…
# ================================

def train_q_learning(env: gym.Env,
                     num_episodes: int = 5000,
                     alpha: float = 0.1,
                     gamma: float = 0.99,
                     epsilon: float = 1.0,
                     epsilon_decay: float = 0.995,
                     epsilon_min: float = 0.01) -> Tuple[np.ndarray, List[float]]:
    """
    Q-Learningã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’
    """
    try:
        n_actions = env.action_space.n
        n_states = env.observation_space.n
        Q = np.zeros((n_states, n_actions))
        rewards = []

        for episode in range(num_episodes):
            state = env.reset()
            done = False
            total_reward = 0

            while not done:
                if np.random.rand() < epsilon:
                    action = env.action_space.sample()
                else:
                    action = np.argmax(Q[state])

                next_state, reward, done, info = env.step(action)

                best_next_action = np.argmax(Q[next_state])
                td_target = reward + gamma * Q[next_state][best_next_action]
                td_error = td_target - Q[state][action]
                Q[state][action] += alpha * td_error

                state = next_state
                total_reward += reward

            epsilon = max(epsilon_min, epsilon * epsilon_decay)
            rewards.append(total_reward)

            if (episode + 1) % 500 == 0:
                logger.info(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1}/{num_episodes} - å¹³å‡å ±é…¬: {np.mean(rewards[-500:]):.2f}")

        logger.info("âœ… Q-Learningãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
        return Q, rewards
    except Exception as e:
        logger.error(f"âŒ Q-Learningãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—: {e}")
        return None, []
# ================================
# å­¦ç¿’ãƒ­ã‚°ãƒ»å ±é…¬æ¨ç§»ã®å¯è¦–åŒ–
# ================================

def plot_rewards(rewards: List[float], window: int = 100) -> None:
    """å ±é…¬æ¨ç§»ã‚’å¯è¦–åŒ–ã™ã‚‹"""
    try:
        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
        plt.figure(figsize=(10, 5))
        plt.plot(moving_avg)
        plt.title("Moving Average of Rewards")
        plt.xlabel("Episode")
        plt.ylabel("Average Reward")
        plt.grid()
        plt.show()
    except Exception as e:
        logger.error(f"âŒ å ±é…¬ãƒ—ãƒ­ãƒƒãƒˆå¤±æ•—: {e}")
# ================================
# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­è¾¼
# ================================

def save_q_table_to_drive(Q: np.ndarray, relative_path: str) -> None:
    """Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’Google Driveã«ä¿å­˜ã™ã‚‹"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        joblib.dump(Q, full_path)
        logger.info(f"âœ… Qãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜å®Œäº†: {full_path}")
    except Exception as e:
        logger.error(f"âŒ Qãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜å¤±æ•—: {e}")

def load_q_table_from_drive(relative_path: str) -> np.ndarray:
    """Google Driveã‹ã‚‰Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        return joblib.load(full_path)
    except Exception as e:
        logger.error(f"âŒ Qãƒ†ãƒ¼ãƒ–ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None
# ================================
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ”¯æ´
# ================================

def upload_file_from_local() -> Dict[str, Any]:
    """ãƒ­ãƒ¼ã‚«ãƒ«PCã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    logger.info("ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    uploaded = files.upload()
    logger.info(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {list(uploaded.keys())}")
    return uploaded