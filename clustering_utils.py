# -*- coding: utf-8 -*-
"""clustering_utils.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sE8Oi6GMkSaDzIEd8H9q6YaEfvCdPdW4

1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ»ç’°å¢ƒè¨­å®šï¼ˆãƒ­ã‚®ãƒ³ã‚°å«ã‚€ï¼‰
2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ï¼ˆä¸»ã«æ¨™æº–åŒ–ãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆKMeans, DBSCAN, éšå±¤å‹ãªã©å¯¾å¿œï¼‰
4. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã€ã‚¨ãƒ«ãƒœãƒ¼æ³•ãƒ—ãƒ­ãƒƒãƒˆãªã©ï¼‰
5. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®å¯è¦–åŒ–ï¼ˆ2D/3Dãƒ—ãƒ­ãƒƒãƒˆï¼‰
6. ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ»èª­è¾¼ï¼ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼‰
7. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ”¯æ´
"""

# ================================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ç’°å¢ƒè¨­å®š
# ================================

import os
import joblib
import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from google.colab import drive
from google.colab import files

from typing import Any, Dict, List, Optional, Tuple, Union

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)
# ================================
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†
# ================================

def load_csv_data(file_path: str, features_to_drop: Optional[List[str]] = None) -> pd.DataFrame:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
    try:
        if not os.path.exists(file_path) and '/content/drive' not in file_path:
            file_path = os.path.join('/content/drive/MyDrive/', file_path)

        df = pd.read_csv(file_path)
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: {df.shape}")

        if features_to_drop:
            df = df.drop(columns=features_to_drop)

        return df
    except Exception as e:
        logger.error(f"âŒ CSVãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None

def standardize_data(X: pd.DataFrame) -> pd.DataFrame:
    """ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ï¼ˆå¹³å‡0ã€åˆ†æ•£1ã«ï¼‰"""
    try:
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        logger.info("âœ… ç‰¹å¾´é‡æ¨™æº–åŒ–å®Œäº†")
        return pd.DataFrame(X_scaled, columns=X.columns)
    except Exception as e:
        logger.error(f"âŒ æ¨™æº–åŒ–å¤±æ•—: {e}")
        return None
# ================================
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
# ================================

def train_clustering_model(X: pd.DataFrame, model_type: str = "kmeans",
                           params: Optional[Dict] = None) -> Any:
    """æŒ‡å®šã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’"""
    logger.info(f"ğŸ”„ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹: {model_type}")

    if model_type.lower() == "kmeans":
        model = KMeans()
    elif model_type.lower() == "dbscan":
        model = DBSCAN()
    elif model_type.lower() == "agglomerative":
        model = AgglomerativeClustering()
    else:
        logger.error(f"âŒ æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
        return None

    if params:
        model.set_params(**params)

    model.fit(X)
    logger.info("âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
    return model
# ================================
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
# ================================

def evaluate_clustering_model(X: pd.DataFrame, labels: np.ndarray) -> float:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    try:
        score = silhouette_score(X, labels)
        logger.info(f"ğŸ“ˆ ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {score:.4f}")
        return score
    except Exception as e:
        logger.error(f"âŒ ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—å¤±æ•—: {e}")
        return -1

def plot_elbow_method(X: pd.DataFrame, max_k: int = 10) -> None:
    """KMeansã®æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¢ã™ã‚¨ãƒ«ãƒœãƒ¼æ³•ãƒ—ãƒ­ãƒƒãƒˆ"""
    try:
        distortions = []
        K = range(1, max_k + 1)
        for k in K:
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(X)
            distortions.append(kmeans.inertia_)

        plt.figure(figsize=(8, 4))
        plt.plot(K, distortions, 'bx-')
        plt.xlabel('k')
        plt.ylabel('Distortion')
        plt.title('Elbow Method for Optimal k')
        plt.show()
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ«ãƒœãƒ¼æ³•ãƒ—ãƒ­ãƒƒãƒˆå¤±æ•—: {e}")
# ================================
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœå¯è¦–åŒ–
# ================================

def plot_clusters_2d(X: pd.DataFrame, labels: np.ndarray) -> None:
    """2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’å¯è¦–åŒ–"""
    try:
        pca = PCA(n_components=2)
        components = pca.fit_transform(X)

        df_plot = pd.DataFrame({
            "PC1": components[:, 0],
            "PC2": components[:, 1],
            "Cluster": labels
        })

        fig = px.scatter(df_plot, x="PC1", y="PC2", color=df_plot["Cluster"].astype(str),
                         title="2D PCA Cluster Plot")
        fig.show()
    except Exception as e:
        logger.error(f"âŒ 2Dã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒƒãƒˆå¤±æ•—: {e}")

def plot_clusters_3d(X: pd.DataFrame, labels: np.ndarray) -> None:
    """3æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’å¯è¦–åŒ–"""
    try:
        pca = PCA(n_components=3)
        components = pca.fit_transform(X)

        df_plot = pd.DataFrame({
            "PC1": components[:, 0],
            "PC2": components[:, 1],
            "PC3": components[:, 2],
            "Cluster": labels
        })

        fig = px.scatter_3d(df_plot, x="PC1", y="PC2", z="PC3", color=df_plot["Cluster"].astype(str),
                            title="3D PCA Cluster Plot")
        fig.show()
    except Exception as e:
        logger.error(f"âŒ 3Dã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒƒãƒˆå¤±æ•—: {e}")
# ================================
# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ»èª­è¾¼
# ================================

def save_model_to_drive(model: Any, relative_path: str) -> None:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’Google Driveã«ä¿å­˜"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        joblib.dump(model, full_path)
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {full_path}")
    except Exception as e:
        logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å¤±æ•—: {e}")

def load_model_from_drive(relative_path: str) -> Any:
    """Google Driveã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    try:
        full_path = os.path.join('/content/drive/MyDrive/', relative_path)
        return joblib.load(full_path)
    except Exception as e:
        logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None
# ================================
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ”¯æ´
# ================================

def upload_file_from_local() -> Dict[str, Any]:
    """ãƒ­ãƒ¼ã‚«ãƒ«PCã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    logger.info("ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    uploaded = files.upload()
    logger.info(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {list(uploaded.keys())}")
    return uploaded