# -*- coding: utf-8 -*-
"""data_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TbhOBH229y2ekuT35e_at5LWovvFYIic
"""

# ================================================
# data_utils.py
# データ前処理・データ操作の共通関数集
# ================================================

import pandas as pd
import numpy as np
import os
import logging
from typing import Optional, List, Tuple, Union
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# ログ設定
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ================================================
# データロード系関数
# ================================================

def load_csv_data(file_path: str, target_column: str, features_to_drop: Optional[List[str]] = None) -> Tuple[pd.DataFrame, pd.Series]:
    """
    CSVファイルを読み込み、特徴量Xと目的変数yに分割する関数

    Args:
        file_path (str): CSVファイルのパス
        target_column (str): 目的変数のカラム名
        features_to_drop (List[str], optional): ドロップしたい特徴量リスト

    Returns:
        Tuple[pd.DataFrame, pd.Series]: 特徴量X、目的変数y
    """
    try:
        if not os.path.exists(file_path):
            logger.error(f"❌ ファイルが存在しません: {file_path}")
            return None, None

        df = pd.read_csv(file_path)
        logger.info(f"✅ データ読み込み成功: {file_path}")
        logger.info(f"🔍 データ概要:\n{df.describe().T}")

        if features_to_drop:
            df = df.drop(columns=features_to_drop)

        if target_column not in df.columns:
            logger.error(f"❌ 指定された目的変数 '{target_column}' がデータに存在しません")
            return None, None

        X = df.drop(columns=[target_column])
        y = df[target_column]

        return X, y

    except Exception as e:
        logger.error(f"❌ データ読み込みエラー: {e}")
        return None, None

# ================================================
# 欠損値処理・カテゴリ変数エンコード系関数
# ================================================

def fill_missing_values(X: pd.DataFrame, strategy: str = "mean") -> pd.DataFrame:
    """
    欠損値を補完する関数

    Args:
        X (pd.DataFrame): 入力データフレーム
        strategy (str, optional): 補完方法（"mean" または "median"）

    Returns:
        pd.DataFrame: 補完後のデータフレーム
    """
    try:
        if strategy == "mean":
            X = X.fillna(X.mean())
            logger.info("✅ 欠損値を平均値で補完しました")
        elif strategy == "median":
            X = X.fillna(X.median())
            logger.info("✅ 欠損値を中央値で補完しました")
        else:
            logger.warning("⚠️ 無効なstrategyが指定されたため、平均値で補完しました")
            X = X.fillna(X.mean())

        return X

    except Exception as e:
        logger.error(f"❌ 欠損値補完エラー: {e}")
        return X


def encode_categorical(X: pd.DataFrame, drop_first: bool = True) -> pd.DataFrame:
    """
    カテゴリ変数をOne-Hotエンコーディングする関数

    Args:
        X (pd.DataFrame): 入力データフレーム
        drop_first (bool, optional): ダミー変数落とし (デフォルトTrue)

    Returns:
        pd.DataFrame: エンコード後のデータフレーム
    """
    try:
        cat_columns = X.select_dtypes(include=["object", "category"]).columns

        if len(cat_columns) == 0:
            logger.info("🔍 カテゴリ変数は検出されませんでした")
            return X

        logger.info(f"🔄 カテゴリ変数をエンコードします: {list(cat_columns)}")
        X_encoded = pd.get_dummies(X, columns=cat_columns, drop_first=drop_first)

        return X_encoded

    except Exception as e:
        logger.error(f"❌ カテゴリ変数エンコードエラー: {e}")
        return X

# ================================================
# データ分割・スケーリング系関数
# ================================================

def split_data(X: pd.DataFrame, y: pd.Series,
               test_size: float = 0.2,
               validation_size: float = 0.0,
               random_state: int = 42
               ) -> Union[
                   Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series],
                   Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]
               ]:
    """
    学習・検証・テスト用にデータを分割する関数

    Args:
        X (pd.DataFrame): 特徴量データ
        y (pd.Series): 目的変数
        test_size (float): テストデータの割合
        validation_size (float): 検証データの割合（0の場合は検証データなし）
        random_state (int): ランダムシード

    Returns:
        Tuple: 分割後のデータセット（学習/検証/テスト）
    """
    try:
        if validation_size > 0:
            X_train, X_temp, y_train, y_temp = train_test_split(
                X, y, test_size=test_size + validation_size, random_state=random_state
            )
            valid_ratio = validation_size / (test_size + validation_size)
            X_valid, X_test, y_valid, y_test = train_test_split(
                X_temp, y_temp, test_size=1-valid_ratio, random_state=random_state
            )
            logger.info(f"✅ データ3分割完了 - Train:{X_train.shape}, Valid:{X_valid.shape}, Test:{X_test.shape}")
            return X_train, X_valid, X_test, y_train, y_valid, y_test
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            logger.info(f"✅ データ2分割完了 - Train:{X_train.shape}, Test:{X_test.shape}")
            return X_train, X_test, y_train, y_test

    except Exception as e:
        logger.error(f"❌ データ分割エラー: {e}")
        return None


def scale_data(X_train: pd.DataFrame,
               X_test: pd.DataFrame,
               scaler_type: str = "standard") -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    データのスケーリングを行う関数

    Args:
        X_train (pd.DataFrame): 学習データ
        X_test (pd.DataFrame): テストデータ
        scaler_type (str): スケーラーの種類（"standard" または "minmax"）

    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: スケーリング後の学習・テストデータ
    """
    try:
        if scaler_type == "standard":
            scaler = StandardScaler()
            logger.info("✅ StandardScalerでスケーリングを実施しました")
        elif scaler_type == "minmax":
            scaler = MinMaxScaler()
            logger.info("✅ MinMaxScalerでスケーリングを実施しました")
        else:
            logger.warning(f"⚠️ 無効なscaler_typeが指定されたため、StandardScalerを使用します")
            scaler = StandardScaler()

        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)

        return X_train_scaled, X_test_scaled

    except Exception as e:
        logger.error(f"❌ スケーリングエラー: {e}")
        return X_train, X_test

# ================================================
# 補助ユーティリティ関数
# ================================================

def show_data_summary(X: pd.DataFrame, y: Optional[pd.Series] = None) -> None:
    """
    データの基本情報（形状、欠損値、統計量）を表示する関数

    Args:
        X (pd.DataFrame): 特徴量データ
        y (pd.Series, optional): 目的変数データ（あれば）
    """
    try:
        logger.info(f"🗂️ 特徴量データ形状: {X.shape}")
        if y is not None:
            logger.info(f"🎯 目的変数データ形状: {y.shape}")

        logger.info("🔍 特徴量データ欠損値サマリー:")
        logger.info(f"\n{X.isnull().sum()[X.isnull().sum() > 0]}")

        logger.info("📈 特徴量データ統計量:")
        logger.info(f"\n{X.describe().T}")

    except Exception as e:
        logger.error(f"❌ データサマリー出力エラー: {e}")

